Inpaper, a skew and uni-/bi-modal extension ofStudent-t
distribution is considered.model is more flexible and has wider ranges
of skewness and kurtosis another skew distributions in literature. Fisher
information matrix forproposed model and some submodels are derived.
With a simulation study and some real data sets, applicability ofproposed
models are illustrated.

A mixture of Extreme Ranked Set Sampling (ERSS) and Median
Ranked Set Sampling (MRSS) is introduced to obtain a more representative
sample using three out of five number summary statistics [i.e., Minimum,
Median and Maximum].proposed sampling scheme provides unbiased
estimator of meansymmetric population and gives moderate efficiency
for both symmetric and asymmetric populations under perfect as well as
imperfect rankings. Expressionsbias and asymptotic variance are presented.
A simulation study isconducted to observeperformance of proposed estimator. Application of proposed sampling scheme is illustrated
through a real life example.

Instudy, a stochastic process X(t), which describes an inventory
model of type (s, S) is considered inpresence of heavy tailed
demands with infinite variance.aim ofstudy is observingimpact
of regularly varying demand distributions with infinite variance on 
stochastic process X(t).main motivation ofwork is,publication
by Geluk [Proceedings ofAmerican Mathematical Society 125 (1997)
3407–3413] where he provided a special asymptotic expansionrenewal
function generated by regularly varying random variables. Two term asymptotic
expansion forergodic distribution function ofprocess X(t) is
obtained based onmain results proposed by Geluk [Proceedings of 
American Mathematical Society 125 (1997) 3407–3413]. Finally, weak convergence
theorem forergodic distribution ofprocess is proved by
using Karamata theory.

We consider a problem of checking whethercoefficient of
scale and location function is a constant. Bothscale and location
functions are modeled as single-index models. Two test statistics based on
Kolmogorov–Smirnov and Cramér–von Mises type functionals ofdifference
ofempirical residual processes are proposed.asymptotic distribution
ofestimatorsingle-index parameter is derived, andempirical
distribution function of residuals is shown to converge to a Gaussian
process. Moreover,proposed test statisticsbe able to detect local alternatives
that converge to zero at a parametric convergence rate. A bootstrap
procedure is further proposed to calculate critical values. Simulation studies
and a real data analysis are conducted to demonstrateperformance of
proposed methods.

Variable dimensional problems, where not onlyparameters,
but alsonumber of parameters are random variables, pose serious challenge
to Bayesians. Although in principleReversible Jump Markov Chain
Monte Carlo (RJMCMC) methodology is a response to such challenges,
dimension-hopping strategies need not be always convenientpractical implementation,
particularly because efficient “move-types” having reasonable
acceptance rates are often difficult to devise.
Inarticle, we propose and develop a novel and general dimensionhopping
MCMC methodology thatupdate allparameters as well as
number of parameters simultaneously using simple deterministic transformations
of some low-dimensional (often one-dimensional) random variable.
This methodology, which has been inspired by Transformation based MCMC
(TMCMC) of (Stat. Mehodol. (2014) 16 100–116), facilitates great speed
in terms of computation time and provides reasonable acceptance rates and
mixing properties. Quite importantly, our approach provides a natural way
to automatemove-types in variable dimensional problems. We refer to
this methodology as Transdimensional Transformation based Markov Chain
Monte Carlo (TTMCMC). Comparisons with RJMCMC in gamma and normal
mixture examples demonstrate far superior performance of TTMCMC in
terms of mixing, acceptance rate, computational speed and automation. Furthermore,
we demonstrate good performance of TTMCMC in multivariate
normal mixtures, evendimension as large as 20. To our knowledge, there
exists no application of RJMCMCsuch high-dimensional mixtures.
As by-products of our effort ondevelopment of TTMCMC, we propose
a novel methodology to summarizeposterior distributions of
mixture densities, providing a way to obtainmode ofposterior distribution
ofdensities andassociated highest posterior density credible
regions. Based on our method, wepropose a criterion to assess convergence
of variable-dimensional algorithms. These methods of summarization
and convergence assessment are applicable to general problems, not just to
mixtures.

It is well known thatuncertainty inestimation of parameters
producesunderestimation ofmean square error (MSE) both for
in-sample and out-of-sample estimation. Instate space framework, this
problemaffect confidence intervalssmoothed estimates and forecasts,
which are generally built by state vector predictors that use estimated
model parameters. In order to correctproblem,paper proposes and
compares parametric and nonparametric bootstrap methods based on procedures
usually employed to calculate theMSE incontext of forecasting and
smoothing in state space models.comparisons are performed through an
extensive Monte Carlo study which illustrates, empirically,bias reduction
inestimation of MSEprediction and smoothed estimates using
 bootstrap approaches.finite sample properties ofbootstrap procedures
are analyzedGaussian and non-Gaussian assumptions oferror
term.procedures areapplied to real time series, leading to satisfactory
results.

Non-linear mixed models are useful in many practical longitudinal
data problems, especially when they are derived as solutions to differential
equations generated by subject matter theoretical considerations. When this
underlying rationale is not available, practitioners are faced withdilemma
of choosing a model fromnumerous ones available inliterature. 
situation is even worsemessy data where interpretation and computational
problems are frequent.iscase with a pilot observational study conducted
atSchool of Medicine ofUniversity of São Paulo in which a
new method to estimatetime since death (post-mortem interval—PMI)
is proposed. In particular,attenuation ofdensity of intra-cardiac hypostasis
(concentration of red cells invascular system by gravity) obtained
from a series of tomographic images was observed inthoraces of
21 bodies of hospitalized patients with known time of death.images were
obtained at different instants and not always atsame conditionseach
body, generating a set of messy data. Incontext, we consider three ad
hoc models to analysedata, commenting onadvantages and caveats
of each approach.

We investigateequivalence of dynamic and static asset allocations
incase whereprice process of a risky asset is driven by a
Poisson process. Under some mild conditions, we obtain a necessary and sufficient
condition forequivalence of dynamic and static asset allocations.
In addition, we provide a simple sufficient condition forequivalence.

Financial returns are known to be nonnormal and tend to have fattailed
distribution. Also,dependence of large values in a stochastic process
is an important topic in risk, insurance and finance. Inpresence of
missing values, we deal withasymptotic properties of a simple “median”
estimator oftail index based on random variables withheavy-tailed
distribution function and certain dependence amongextremes.Weak consistency
and asymptotic normality ofproposed estimator are established.
 estimator is a special case of a well-known estimator defined in Bacro
and Brito [Statistics & Decisions 3 (1993) 133–143].advantage of 
estimator is its robustness against deviations and compared to Hill’s, it is less
affected byfluctuations related tomaximum ofsample or by
presence of outliers. Several examples are analyzed in order to support
proofs.

Whole robustness is a nice property to havestatistical models.
It implies thatimpact of outliers gradually vanishes as they approach plus
or minus infinity. So far,Bayesian literature provides results that ensure
whole robustness forlocation-scale model. Inpaper, we make two
contributions. First, we generaliseresults to attain whole robustness in
simple linear regression throughorigin, which is a necessary step towards
resultsgeneral linear regression models.We allowvariance oferror
term to depend onexplanatory variable.flexibility leads tosecond
contribution: we provide a simple Bayesian approach to robustly estimate
finite population means and ratios.strategy to attain whole robustness is
simple since it lies in replacingtraditional normal assumption onerror
term by a super heavy-tailed distribution assumption. As a result, users can
estimateparameters as usual, usingposterior distribution.

Dutta and Bhattacharya (Statistical Methodology 16 (2014) 100–
116) as an efficient alternative toMetropolis–Hastings algorithm, especially
in high dimensions.main advantage ofalgorithm is that it simultaneously
updates all components of a high dimensional parameter using
appropriate move types defined by deterministic transformation of a single
random variable.results in reduction in time complexity at each step of
chain and enhancesacceptance rate.
Inpaper, we first provide a brief review ofoptimal scaling theory
for various existing MCMC approaches, comparing and contrasting them
withcorresponding TMCMC approaches. optimal scaling ofsimplest
form of TMCMC, namely additive TMCMC, has been studied extensively
forGaussian proposal density in Dey and Bhattacharya (2017a).
Here, we discuss diffusion-based optimal scaling behavior of additive TMCMC
for non-Gaussian proposal densities—in particular, uniform, Student’s
t and Cauchy proposals. Although we could not formally prove our diffusion
result forCauchy proposal, simulation based results lead us to conjecture
that at leastrecipeobtaining general optimal scaling and optimal acceptance
rate holds forCauchy case as well. Weconsider diffusion
based optimal scaling of TMCMC whentarget density is discontinuous.
Such non-regular situations have been studied incase of Random Walk
Metropolis Hastings (RWMH) algorithm by Neal and Roberts (Methodology
and Computing in Applied Probability 13 (2011) 583–601) using expected
squared jumping distance (ESJD), butdiffusion theory based scaling has
not been considered.
We compare our diffusion based optimally scaled TMCMC approach with
 ESJD based optimally scaled RWMwith simulation studies involving several
target distributions and proposal distributions includingchallenging
Cauchy proposal case, showing that additive TMCMC outperforms RWMH
in almost all cases considered.

pressing needimproved methodsanalysing and coping
with big data has opened up a new area of researchstatisticians. Image
analysis is an area where there is typically a very large number of data points
to be processed per image, and often multiple images are captured over time.
These issues make it challenging to design methodology that is reliable and
yet still efficient enough to be of practical use. One promising emerging approach
forproblem is to reduceamount of data that actually has to
be processed by extracting what we call coresets fromfull dataset; analysis
is then based oncoreset rather thanwhole dataset. Coresets are
representative subsamples of data that are carefully selected via an adaptive
sampling approach. We propose a new approach called coreset variational
Bayes (CVB)mixture modelling;is an algorithm whichperform
a variational Bayes analysis of a dataset based on just an extracted coreset of
data. We apply our algorithm to weed image analysis.

Inpaper, we studychange point problem forskew
normal distribution model fromview of model selection problem. 
detection procedure based onmodified information criterion (MIC) for
change problem is proposed. Such a procedure has advantage in detecting 
changes in early and late stage of a data comparing toone based on 
traditional Schwarz information criterion which is well known as Bayesian
information criterion (BIC) by consideringcomplexity ofmodels. Due
todifficulty in derivinganalytic asymptotic distribution oftest
statistic based onMIC procedure,bootstrap simulation is provided to
obtaincritical values atdifferent significance levels. Simulations are
conducted to illustratecomparisons of performance between MIC, BIC
and likelihood ratio test (LRT). Such an approach is applied on two stock
market data sets to indicatedetection procedure.

Birnbaum–Saunders (BS) distribution has been largely studied
and applied. A random variable with BS distribution is a transformation of
another random variable with standard normal distribution. Generalized BS
distributions are obtained whennormally distributed random variable is
replaced by another symmetrically distributed random variable.allows
us to obtain a wide class of positively skewed models with lighter and heavier
tails thanBS model. Its failure rate admits several shapes, including
unimodal case, with its change-point being able to be useddifferent
purposes.example, to establishreduction in a dose, and then in 
cost ofmedical treatment. We analyzefailure rates of generalized
BS distributions obtained bylogistic, normal and Student-t distributions,
considering their shape and change-point, estimating them, evaluating their
robustness, assessing their performance by simulations, and applyingresults
to real data from different areas.

log-linear Birnbaum–Saunders model has been widely used
in empirical applications. We introduce an extension ofmodel based on
a recently proposed version ofBirnbaum–Saunders distribution which is
more flexible thanstandard Birnbaum–Saunders law since its density may
assume both unimodal and bimodal shapes. Wehow to perform point
estimation, interval estimation and hypothesis testing inferences onparameters
that indexregression model we propose.Wepresent a number
of diagnostic tools, such as residual analysis, local influence, generalized
leverage, generalized Cook’s distance and model misspecification tests. We
investigateusefulness of model selection criteria andaccuracy of prediction
intervals forproposed model. Results of Monte Carlo simulations
are presented. Finally, wepresent and discuss an empirical application.

Consider a supercritical branching random walk onreal line.
consistent maximal displacement issmallest ofdistances between
trajectories followed by individuals atnth generation andboundary
ofprocess. Fang and Zeitouni, and Faraud, Hu and Shi proved that under
some integrability conditions,consistent maximal displacement grows almost
surely at rate λ
∗
n1/3some explicit constant λ
∗. We obtain here a
necessary and sufficient condition forasymptotic behaviour to hold.

Indata analysis from multiple repairable systems, it is usual to
observe both different truncation times and heterogeneity amongsystems.
Among other reasons,latter is caused by different manufacturing lines
and maintenance teams ofsystems. Inpaper, a hierarchical model
is proposed forstatistical analysis of multiple repairable systems under
different truncation times. A reparameterization ofpower law process is
proposed in order to obtain a quasi-conjugate bayesian analysis. An empirical
Bayes approach is used to estimate model hyperparameters.uncertainty
inestimate of these quantities are corrected by using a parametric bootstrap
approach.results are illustrated in a real data set of failure times of
power transformers from an electric company in Brazil.

We studyrate of convergence incelebrated Shape Theorem
in first-passage percolation, obtainingprecise asymptotic rate of decay
forprobability of linear order deviations under a moment condition. Our
results are presented from a temporal perspective and complement previous
work bysame author, in whichrate of convergence was studied from
standard spatial perspective.

Inpaper, we present a regression model whereresponse
variable is a count data that follows aWaring distribution. waring regression
model allowsanalysis of phenomena whereGeometric regression
model is inadequate, becauseprobability of success on each trial, p, is
differenteach individual and p has an associated distribution. Estimation
is performed by maximum likelihood, throughmaximization of 
Q-function using EM algorithm. Diagnostic measures are calculatedthis
model. To illustrateresults, an application to real data is presented. Some
specific details are given inAppendix ofpaper.

We study a rank based univariate two-sample distribution-free
test.test statistic isdifference betweenaverage of between-group
rank distances andaverage of within-group rank distances.test statistic
is closely related totwo-sample Cramér–von Mises criterion. They are
different empirical versions of a same quantitytestingequality of two
population distributions. Although they may be differentfinite samples,
they sharesame expected value, variance and asymptotic properties. 
advantage ofnew rank based test overclassical one is its ease to
generalize tomultivariate case. Rather than usingempirical process
approach, we provide a different easier proof, bringing in a different perspective
and insight. In particular, we applyHájek projection and orthogonal
decomposition technique in derivingasymptotics ofproposed rank
based statistic. A numerical study compares power performance ofrank
formulation test with other commonly-used nonparametric tests and recommendations
on those tests are provided. Lastly, we propose a multivariate
extension oftest based onspatial rank.

Tadikamalla and Johnson [Biometrika 69 (1982) 461–465] developed
LB distribution to variables with bounded support by considering a
transformation ofstandard Logistic distribution. Inmanuscript, a convenient
parametrization ofdistribution is proposed in order to develop regression
models.distribution, referred to here as L-Logistic distribution,
provides great flexibility and includesuniform distribution as a particular
case. Several properties ofdistribution are studied, and a Bayesian
approach is adopted forparameter estimation. Simulation studies, considering
prior sensitivity analysis, recovery of parameters and comparison of
algorithms, and robustness to outliers are all discussed showing thatresults
are insensitive tochoice of priors, efficiency ofalgorithmMCMC
adopted, and robustness ofmodel when compared withbeta distribution.
Applications to estimatevulnerability to poverty and to explain 
anxiety are performed.results to applicationsthatL-Logistic
regression models provide a better fit thancorresponding beta regression
models.

We proveexistence and uniqueness ofsolution of backward
stochastic variational inequalities with respect to fractional Brownian
motion and with non-Lipschitz coefficient. We assume thatH >1/2.

use of Markov random field (MRF) models has proven to
be a fruitful approach in a wide range of image processing applications. It
allows local texture information to be incorporated in a systematic and unified
way and allows statistical inference theory to be applied giving rise to novel
output summaries and enhanced image interpretation. A great advantage of
such low-level approaches is that they lead to flexible models, whichbe
applied to a wide range of imaging problems withoutneedsignificant
modification.
This paper proposes and exploresuse of conditional MRF models
for situations where multiple images are to be processed simultaneously, or
where only a single image is to be reconstructed and a sequential approach
is taken. Althoughcoupling of image intensity values is a special case
of our approach,main extension over previous proposals is to allow 
direct coupling of other properties, such as smoothness or texture.is
achieved using a local modulating function which adjustsinfluence of
global smoothing withoutneeda fully inhomogeneous prior model.
Several modulating functions are considered and a detailed simulation study,
motivated by remote sensing applications in archaeological geophysics, of
conditional reconstruction is presented.results demonstrate that a substantial
improvement inquality ofimage reconstruction, in terms of
errors and residuals,be achieved usingapproach, especially at locations
with rapid changes inunderlying intensity.

Via a special transform and by usingtechniques ofMalliavin
calculus, we analyzedensity ofsolution to a stochastic differential
equation with unbounded drift.

We consider a Jackson network in a general heavy traffic diffusion
regime withα-parametrization. Weassume that each customer
may abandonsystem while waiting. Wethat inregime 
queue-length process converges to a multi-dimensional regulated Ornstein–
Uhlenbeck process.

We revisit a shape inversion formula derived by Panaretos in 
context of a particle density estimation problem with unknown rotation of 
particle. A distribution is presented which imitates, or “fakes”,uniformity
or Haar distribution that is part of that formula.

Stochastic monotonicity is a well-known partial order relation
between probability measures defined onsame partially ordered set.
Strassen theorem establishes equivalence between stochastic monotonicity
andexistence of a coupling compatible with respect topartial order.
We considercase of a countable set and introduceclass of finitely decomposable
flows on a directed acyclic graph associated topartial order.
Wethat a probability measure stochastically dominates another probability
measure if and only if there exists a finitely decomposable flow having
divergence given bydifference oftwo measures. We illustrateresult
with some examples.

We consider a random object that is associated with both random
walks and random media, specifically,superposition of a configuration of
subcritical Bernoulli percolation on an infinite connected graph andtrace
ofsimple random walk onsame graph. We investigate asymptotics
fornumber of vertices ofenlargement oftrace ofwalk until
a fixed time, whentime tends to infinity.process is more highly
self-interacting thanrange of random walk, which yields difficulties. We
show a law of large numbers on vertex-transitive transient graphs. We compare
process on a vertex-transitive graph withprocess on a finitely
modified graph oforiginal vertex-transitive graph and their behaviors
are similar.Wethatprocess fluctuates almost surely on a certain
non-vertex-transitive graph. Ontwo-dimensional integer lattice, by investigating
size ofboundary oftrace, we give an estimatevariances
ofprocess implying a law of large numbers.We give an example of
a graph with unbounded degrees on whichprocess behaves in a singular
manner. As by-products, some results forrange andboundary, which
will be of independent interest, are obtained.

This paper discusses a pth-order dependence-driven random coefficient
integer-valued autoregressive time series model (DDRCINAR(p)).
Stationarity and ergodicity properties are proved. Conditional least squares,
weighted least squares and maximum quasi-likelihood are used to estimate
model parameters. Asymptotic properties ofestimators are presented.
performances of these estimators are investigated and compared via simulations.
In certain regions ofparameter space, simulative analysis shows
that maximum quasi-likelihood estimators perform better thanestimators
of conditional least squares and weighted least squares in terms ofproportion
of within- estimates. At last,model is applied to two real data
sets.

epidemic process on a graph is consideredwhich infectious
contacts occur at rate which depends on whether a susceptible is infected
forfirst time or not.WethatVasershtein coupling extends
if and only if secondary infections occur at rate which is greater than that of
initial ones. Nonetheless wethat, with respect toprobability of occurrence
of an infinite epidemic,said proviso may be dropped regarding
totally asymmetric process in one dimension, thus settling inaffirmative
this special case ofconjecturearbitrary graphs due to [Ann. Appl.
Probab. 13 (2003) 669–690].

Point processes are one ofmost commonly encountered observation
processes in Spatial Statistics. Model-based inference them depends
onlikelihood function. Inmost standard setting of Poisson processes,likelihood depends onintensity function, andnot be computed
analytically. A number of approximating techniques have been proposed
to handledifficulty. Inpaper, we review recent work on exact
solutions that solveproblem without resorting to approximations. 
presentation concentrates more heavily on discrete time butconsiders
continuous time.solutions are based on model specifications that impose
smoothness constraints onintensity function. Wereview approaches
to include a regression component and different ways to accommodate it
while accountingadditional heterogeneity. Applications are provided to
illustrateresults. Finally, we discuss possible extensions to account for
discontinuities and/or jumps inintensity function.

Finite mixture models and their extensions to Markov mixture and
mixture of experts models are very popular in analysing data of various kind.
A challenge these models is choosingnumber of components based
on marginal likelihoods.present paper suggests two innovative, generic
bridge sampling estimators ofmarginal likelihood that are based on constructing
balanced importance densities fromconditional densities arising
during Gibbs sampling.full permutation bridge sampling estimator is derived
from considering all possible permutations ofmixture labelsa
subset of these densities. Fordouble random permutation bridge sampling
estimator, two levels of random permutations are applied, first to permute 
labels ofMCMC draws and second to randomly permutelabels of
conditional densities arising during Gibbs sampling. Various applications
show very good performance of these estimators in comparison to importance
and to reciprocal importance sampling estimators derived fromsame importance
densities.

We studylimiting behavior ofone-at-a-time Gibbs sampler
forintrinsic conditional autoregressive model with centering on 
fly.intrinsic conditional autoregressive model is widely used as a prior
for random effects in hierarchical modelsspatial modeling.model
is defined by full conditional distributions that imply an improper joint “density”
with a multivariate Gaussian kernel and a singular precision matrix. To
guarantee propriety ofposterior distribution, usually atend of each
iteration ofGibbs samplerrandom effects are centered to sum to zero
in what is widely known as centering onfly. Whileworks well in
practice,informal computational way to recenterrandom effects obscures
their implied prior distribution and preventsdevelopment of formal
Bayesian procedures. Here wethatimplied prior distribution, that
is,limiting distribution ofone-at-a-time Gibbs sampler forintrinsic
conditional autoregressive model with centering onfly is a singular
Gaussian distribution with a covariance matrix that isMoore–Penrose
inverse ofprecision matrix.result has important implications for
development of formal Bayesian procedures such as reference priors and
Bayes-factor-based model selectionspatial models.

Bayesian hypothesis testing is re-examined fromperspective
of an a priori assessment oftest statistic distribution underalternative.
By assessingdistribution of an observable test statistic, rather than
prior parameter values, we revisitseminal paper of Edwards, Lindman
and Savage (Psychol. Rev. 70 (1963) 193–242). There are a number of important
take-aways from comparingBayesian paradigm via Bayes factors to
frequentist ones. We provide examples where evidencea Bayesian strikingly
supportsnull, but leads to rejection under a classical test. Finally,
we conclude with directionsfuture research.

Observation and parameter driven models are commonly used in
literature to analyse time series of counts. Inpaper, we study 
characteristics of a variety of models and point outmain differences and
similarities among these procedures, concerning parameter estimation, model
fitting and forecasting. Alternatively toliterature, all inference was performed
underBayesian paradigm.models are fitted with a latent
AR(p) process inmean, which accountsautocorrelation indata.
An extensive simulation study shows thatestimates forcovariate parameters
are remarkably similar acrossdifferent models. However, estimates
for autoregressive coefficients and forecasts of future values depend
heavily onunderlying process which generatesdata. A real data set of
bankruptcy inUnited States isanalysed.

Educational assessment usually considers a contextual questionnaire
to extract relevant information fromapplicants.may include
items related to socio-economical profile as well as items to extract other
characteristics potentially related to applicant’s performance intest.
A careful analysis ofquestionnaires jointly withtest’s results may evidence
important relations between profiles and test performance.most
coherent way to performtask in a statistical context is to useinformation
fromquestionnaire to help explainvariability ofabilities
in a joint model-based approach. Nevertheless,responses toquestionnaire
typically present missing values which, in some cases, may be missing
not at random.paper proposes a statistical methodology to model 
abilities in dichotomous IRT models usinginformation ofcontextual
questionnaires via linear regression.proposed methodology models 
missing data jointly withallobserved data, which allows forestimation
offormer.missing data modelling is flexible enough to allow
specification of missing not at random structures. Furthermore, even if
those structures are not assumed a priori, theybe estimated from 
posterior results when assuming missing (completely) at random structures
a priori. Statistical inference is performed underBayesian paradigm via
an efficient MCMC algorithm. Simulated and real examples are presented to
investigateefficiency and applicability ofproposed methodology.

Multivariate options are adequate toolsmulti-asset risk management.
pricing models derived frompioneer Black and Scholes
method undermultivariate case consider thatasset-object prices follow
a Brownian geometric motion. However,construction of such methods
imposes some unrealistic constraints onprocess of fair option calculation,
such as constant volatility overmaturity time and linear correlation
betweenassets. Therefore,paper aims to price and analyze
fair price behavior ofcall-on-max (bivariate) option considering
marginal heteroscedastic models with dependence structure modeled via
copulas. Concerning inference, we adopt a Bayesian perspective and computationally
intensive methods based on Monte Carlo simulations via Markov
Chain (MCMC). A simulation study examinesbias, androot mean
squared errors ofposterior means forparameters. Real stocks prices
of Brazilian banks illustrateapproach. Forproposed method is verified
effects of strike and dependence structure onfair price ofoption.
resultsthatprices obtained by our heteroscedastic model approach
and copulas differ substantially fromprices obtained bymodel
derived from Black and Scholes. Empirical results are presented to argue the
advantages of our strategy.

primary goal ofpaper is to introducezero-modified
Poisson–Lindley regression model as an alternative to model overdispersed
count data exhibiting inflation or deflation of zeros inpresence of covariates.
zero-modification is incorporated by considering that a zerotruncated
process produces positive observations and consequently,proposed
modelbe fitted without any previous information aboutzeromodification
present in a given dataset. A fully Bayesian approach based on
g-prior method has been consideredinference concerns. An intensive
Monte Carlo simulation study has been conducted to evaluateperformance
ofdeveloped methodology andmaximum likelihood estimators.
proposed model was considered foranalysis of a real dataset
onnumber of bids received by 126 U.S. firms between 1978–1985, and
impact of choosing different prior distributions forregression coefficients
has been studied. A sensitivity analysis to detect influential points has
been performed based onKullback–Leibler divergence. A general comparison
with some well-known regression modelsdiscrete data has been
presented.

This article proposes a calibration schemeBayesian testing
that coordinates analytically-derived statistical performance considerations
with expert opinion. In other words,scheme is effective and meaningful
for incorporating objective elements into subjective Bayesian inference.
It explores a novel roledefault priors as anchorscalibration rather
than substitutesprior knowledge. Ideas are developeduse with multiplicity
adjustments in multiple-model contexts, and to addressissue of
prior sensitivity of Bayes factors. Alongway,performance properties
of an existing multiplicity adjustment related toPoisson distribution
are clarified theoretically. Connections ofoverall calibration scheme to
Schwarz criterion areexplored.proposed framework is examined
and illustrated on a number of existing data sets related to problems in
clinical trials, forensic pattern matching, and log-linear models methodology.

This paper focuses on Bayesian estimation ofparameters and
reliability function ofpower Lindley distribution by using various symmetric
and asymmetric loss functions. Assuming suitable priors onparameters,
Bayes estimates are derived by using squared error, linear exponential
(linex) and general entropy loss functions. Since, under these loss functions,
Bayes estimates ofparameters do not have closed forms we use lindley’s
approximation technique to calculateBayes estimates. Moreover, we obtain
Bayes estimates ofparameters using aMarkov Chain Monte Carlo
(MCMC) method. Simulation studies are conducted in order to evaluate the
performances ofproposed estimators underconsidered loss functions.
Finally, analysis of a real data set is presentedillustrative purposes.

Inarticle, we consider modelstime-to-event data obtained
from experiments in which stress levels are altered at intermediate stages during
observation period. These experiments, known as step-stress tests, belong
tolarger class of accelerated tests used extensively inreliability
literature.analysis of data from step-stress tests largely relies onpopular
cumulative exposure model. However, despite its simple form,utility
ofmodel is limited, as it is assumed thathazard function ofunderlying
distribution is discontinuous atpoints at whichstress levels are
changed, which may not be very reasonable. Due todeficiency, Kannan
et al. (Journal of Applied Statistics 37 (2010b) 1625–1636) introducedcumulative
risk model, wherehazard function is continuous. Inpaper,
we propose a class of parametric models based oncumulative risk model
assumingunderlying population contains long-term survivors or ‘cured’
fraction. An EM algorithm to computemaximum likelihood estimators of
unknown parameters is proposed.research is motivated by a study
on altitude decompression sickness.performance of different parametric
models will be evaluated using data fromstudy.

We addressissue of performing testing inference in small samples
inclass of beta regression models. We considerlikelihood ratio
test and its standard bootstrap version. Weconsider two alternative
resampling-based tests. One of them usesbootstrap test statistic replicates
to numerically estimate a Bartlett correction factor thatbe applied to the
likelihood ratio test statistic. By doing so, we avoid estimation of quantities
located intail oflikelihood ratio test statistic null distribution. The
second alternative resampling-based test uses a fast double bootstrap scheme
in which a single second level bootstrapping resample is performedeach
first level bootstrap replication. It delivers accurate testing inferences at a
computational cost that is considerably smaller than that of a standard double
bootstrapping scheme.Monte Carlo results we providethat
standard likelihood ratio test tends to be quite liberal in small samples.
They alsothatbootstrap tests deliver accurate testing inferences
even whensample size is quite small. An empirical application is also
presented and discussed.

Longitudinal zero-inflated count data are widely encountered in
many fields, while modelingcorrelation between measurementsthe
same subject is more challenge due tolack of suitable multivariate joint
distributions.paper studies a novel mean-correlation modeling approach
for longitudinal zero-inflated regression model, solving both problems of
specifying joint distribution and parsimoniously modeling correlations with
no constraint.joint distribution of zero-inflated discrete longitudinal responses
is modeled by a copula model whose correlation parameters are innovatively
represented in hyper-spherical coordinates. To overcomecomputational
intractability in maximizingfull likelihood function ofmodel,
we further propose a computationally efficient pairwise likelihood approach.
We then propose separated mean and correlation regression models to model
these key quantities, such modeling approach canhandle irregularly and
possibly subject-specific times points.resulting estimators are shown to
be consistent and asymptotically normal. Data example and simulations support
effectiveness ofproposed approach.

Inpaper, we present a novel methodology to perform
Bayesian model selection in linear models with heavy-tailed distributions.
We consider a finite mixture of distributions to model a latent variable where
each component ofmixture corresponds to one possible model within the
symmetrical class of normal independent distributions. Naturally,Gaussian
model is one ofpossibilities.allowsa simultaneous analysis
based onposterior probability of each model. Inference is performed
via Markov chain Monte Carlo—a Gibbs sampler with Metropolis–Hastings
stepsa class of parameters. Simulated examples highlightadvantages
ofapproach compared to a segregated analysis based on arbitrarily chosen
model selection criteria. Examples with real data are presented and an
extension to censored linear regression is introduced and discussed.

Present day bio-medical research is pointing towardsfact that
cognizance of gene–environment interactions along with genetic interactions
may help prevent or detainonset of many complex diseases like cardiovascular
disease, cancer, type2 diabetes, autism or asthma by adjustments to
lifestyle.
Inregard, we propose a Bayesian semiparametric model to detect
not onlyroles of genes and their interactions, but alsopossible influence
of environmental variables ongenes in case-control studies. Our
modelaccounts forunknown number of genetic sub-populations via
finite mixtures composed of Dirichlet processes. An effective parallel computing
methodology, developed by us harnessespower of parallel processing
technology to increaseefficiencies of our conditionally independent
Gibbs sampling and Transformation based MCMC (TMCMC) methods.
Applications of our model and methods to simulation studies with biologically
realistic genotype datasets and a real, case-control based genotype
dataset on early onset of myocardial infarction (MI) have yielded quite interesting
results beside providing some insights intodifferential effect of
gender on MI.

We introduce a two-parameter discrete distribution that may have
a zero vertex andbe usefulmodeling overdispersion.discrete
Nielsen distribution generalizesFisher logarithmic (i.e., logarithmic series)
and Stirling type I distributions insense that bothbe considered
displacements ofNielsen distribution. We provide a comprehensive
account ofstructural properties ofnew discrete distribution. We also
show thatNielsen distribution is infinitely divisible.We discuss maximum
likelihood estimation ofmodel parameters and provide a simple method
to find them numerically.usefulness ofproposed distribution is illustrated
by means of three real data sets to prove its versatility in practical
applications.

We consider a new nonparametric rule of classification, inspired
fromclassical moving window rule, that allows forclassification
of spatially dependent functional data containing some completely missing
curves. We investigateconsistency ofclassifier under mild conditions.
practical use ofclassifier will be illustrated through simulation
studies.

While derivations ofcharacterization ofd-variate exchangeable
Marshall–Olkin copula via d-monotone sequences relying on basic
knowledge in probability theory exist inliterature, they contain a myriad
of unnecessary relatively complicated computations. We revisitissue
and provide proofs where all undesired artefacts are removed, thereby exposing
simplicity ofcharacterization. In particular, we give an insightful
analytical derivation ofmonotonicity conditions based onmonotonicity
properties ofsurvival probabilities.

We usedelta method and Stein’s method to derive, under
regularity conditions, explicit upper bounds fordistributional distance
betweendistribution ofmaximum likelihood estimator (MLE) of a
d-dimensional parameter and its asymptotic multivariate normal distribution.
Our bounds apply in situations in whichMLEbe written as a function
of a sum of i.i.d. t-dimensional random vectors. We apply our general bound
to establish a bound formultivariate normal approximation ofMLE
ofnormal distribution with unknown mean and variance.

Consider two parallel systems, say A and B, with respective lifetimes
T1 and T2 wherein independent component lifetimes of each system
follow exponentiated generalized gamma distribution with possibly different
exponential shape and scale parameters.Wehere that T2 is smaller than
T1 with respect tousual stochastic order (reversed hazard rate order) if
vector of logarithm (main vector) of scale parameters of System B
is weakly weighted majorized by that of System A, and ifvector of exponential
shape parameters of System A is unordered mojorized by that of
System B. By means of some examples, wethatabove results can
not be extended tohazard rate and likelihood ratio orders. However, when
scale parameters of each system divide into two homogeneous groups,
we verify thatusual stochastic and reversed hazard rate ordersbe
extended, respectively, tohazard rate and likelihood ratio orders.established
results complete and strengthen some ofknown results in the
literature.

Suppose that π1,π2, . . . , πk be k(≥ 2) independent exponential
populations having unknown location parameters μ1,μ2, . . . , μk and known
scale parameters σ1, . . . , σk. Let μ[k] = max{μ1, . . . , μk
}.selecting the
population associated with μ[k], a class of selection rules (proposed by Arshad
and Misra [Statistical Papers 57 (2016) 605–621]) is considered. We
considerproblem of estimatinglocation parameter μS ofselected
population undercriterion ofLINEX loss function. We consider three
natural estimators δN,1, δN,2 and δN,3 of μS, based onmaximum likelihood
estimators, uniformly minimum variance unbiased estimator (UMVUE)
and minimum risk equivariant estimator (MREE) of μi ’s, respectively. The
uniformly minimum risk unbiased estimator (UMRUE) andgeneralized
Bayes estimator of μS are derived. UnderLINEX loss function, a general
resultimproving a location-equivariant estimator of μS is derived.
Usingresult, estimator better thannatural estimator δN,1 is obtained.
Weshown thatestimator δN,1 is dominated bynatural estimator
δN,3. Finally, we perform a simulation study to evaluate and compare risk
functions among various competing estimators of μS.

Da Paz, Balakrishnan and Bazan [Braz. J. Probab. Stat. 33 (2019),
455–479] introducedL-logistic distribution, studied its properties including
estimation issues and illustrated a data application.note derives a
closed form expressionmoment properties ofdistribution. Some computational
issues are discussed.

We considerstochastic transport equation with a possibly unbounded
Hölder continuous vector field. Well-posedness is proved, namely,
weexistence, uniqueness and strong stability of W1,p-weak solutions.

As high-dimensional and high-frequency data are being collected
on a large scale,development of new statistical models is being pushed
forward. Functional data analysis providesrequired statistical methods to
deal with large-scale and complex data by assuming that data are continuous
functions,example, realizations of a continuous process (curves) or continuous
random field (surfaces), and that each curve or surface is considered
as a single observation. Here, we provide an overview of functional data analysis
when data are complex and spatially correlated. We provide definitions
and estimators offirst and second moments ofcorresponding functional
random variable. We present two main approaches:first assumes
that data are realizations of a functional random field, that is, each observation
is a curve with a spatial component.We call them spatial functional data.
second approach assumes that data are continuous deterministic fields
observed over time. Incase, one observation is a surface or manifold,
and we call them surface time series.these two approaches, we describe
software available forstatistical analysis. Wepresent a data illustration,
using a high-resolution wind speed simulated dataset, as an example of
two approaches.functional data approach offers a new paradigm of
data analysis, wherecontinuous processes or random fields are considered
as a single entity. We considerapproach to be very valuable in the
context of big data.

Despite its common practice, statistical hypothesis testing presents
challenges in interpretation.instance, instandard frequentist framework
there is no control oftype II error. As a result,non-rejection of
null hypothesis (H0) cannot reasonably be interpreted as its acceptance.
We propose thatdilemmabe overcome by using agnostic hypothesis
tests, since theycontroltype I and II errors simultaneously. In order
to makeidea operational, wehow to obtain agnostic hypothesis
in typical models.instance, wehow to build (unbiased) uniformly
most powerful agnostic tests and how to obtain agnostic tests from standard
p-values. Also, we present conditions such thatabove testsbe made
logically coherent. Finally, we present examples of consistent agnostic hypothesis
tests.

To predict time series of counts with small values and remarkable
fluctuations, an available model isr states random environment process
based onnegative binomial thinning operator andgeometric marginal.
However, we argue thataforementioned model may suffer fromfollowing
two drawbacks. First, undercondition of no prior information,
overdispersed property ofgeometric distribution may causepredictions
fluctuate greatly. Second, because ofconstraints onmodel
parameters, some estimated parameters are close to zero in real-data examples,
which may not objectively revealcorrelation relationship.the
first drawback, an r states random environment process based onbinomial
thinning operator andPoisson marginal is introduced.the
second drawback, we propose a generalized r states random environment
integer-valued autoregressive model based onbinomial thinning operator
to model fluctuations of data. Yule–Walker and conditional maximum
likelihood estimates are considered and their performances are assessed via
simulation studies. Two real-data sets are conducted to illustratebetter
performances ofproposed models compared with some existing models.

Inpaper, we studyfinite mixtures of autoregressive processes
assuming thatdistribution of innovations (errors) belongs to the
class of scale mixture of skew-normal (SMSN) distributions.SMSN distributions
allow a simultaneous modeling ofexistence of outliers, heavy
tails and asymmetries indistribution of innovations. Therefore, a statistical
methodology based onSMSN family allows us to use a robust
modeling on some non-linear time series with great flexibility, to accommodate
skewness, heavy tails and heterogeneity simultaneously.existence
of convenient hierarchical representations ofSMSN distributions facilitates
alsoimplementation of an ECME-type of algorithm to perform the
likelihood inference inconsidered model. Simulation studies andapplication
to a real data set are finally presented to illustrateusefulness of
proposed model.

In adaptive crossover design, our goal is to allocate more patients
to a promising treatment sequence.present work contains a very simple
three period crossover designtwo competing treatments whereallocation
in period 3 is done onbasis ofdata obtained fromfirst
two periods. Assuming normality of response variables we use a reliability
functional forchoice between two treatments. We calculateallocation
proportions and their standard errors corresponding topossible treatment
combinations. Wederive some asymptotic results and provide solutions
on related inferential problems. Moreover,proposed procedure is compared
with a possible competitor. Finally, we use a data set to illustrate the
applicability ofproposed design.

interest onanalysis ofzero–one augmented beta regression
(ZOABR) model has been increasing overlast few years. In this
work, we developed a Bayesian inference forZOABR model, providing
some contributions, namely: we exploreduse of Jeffreys-rule and independence
Jeffreys priorsome ofparameters, performing a sensitivity
study of prior choice, comparingBayesian estimates withmaximum
likelihood ones and measuringaccuracy ofestimates under several
scenarios of interest.results indicate, in a general way, that:Bayesian
approach, underJeffreys-rule prior, was as accurate asML one. Also,
different from other approaches, we usepredictive distribution ofresponse
to implement Bayesian residuals. To further illustrateadvantages
of our approach, we conduct an analysis of a real psychometric data set including
a Bayesian residual analysis, where it is shown that misleading inference
can be obtained whendata is transformed. That is, whenzeros
and ones are transformed to suitable values andusual beta regression
model is considered, instead ofZOABR model. Finally, future developments
are discussed.

Inpaper, we introduce a Bayesian approachclustering data
using a sparse finite mixture model (SFMM).SFMM is a finite mixture
model with a large number of components k previously fixed where many
componentsbe empty. Inmodel,number of components kbe
interpreted asmaximum number of distinct mixture components. Then,
we exploreuse of a prior distribution forweights ofmixture
model that take into accountpossibility thatnumber of clusters kc
(e.g., nonempty components)be random and smaller thannumber of
components k offinite mixture model. In order to determine clusters we
develop a MCMC algorithm denominated Split-Merge allocation sampler. In
this algorithm,split-merge strategy is data-driven and was inserted within
algorithm in order to increasemixing ofMarkov chain in relation
tonumber of clusters.performance ofmethod is verified using
simulated datasets and three real datasets.first real data set isbenchmark
galaxy data, while second and third arepublicly available data set
on Enzyme and Acidity, respectively.

We consider estimation ofmulticomponent stress-strength reliability
under progressive Type II censoring underassumption that stress
and strength variables follow Burr XII distributions with a common shape parameter.
Maximum likelihood estimates ofreliability are obtained along
with asymptotic intervals when common shape parameter may be known or
unknown. Bayes estimates arederived undersquared error loss function
using different approximation methods. Further, we obtain exact Bayes
and uniformly minimum variance unbiased estimates ofreliabilitythe
case common shape parameter is known.highest posterior density intervals
areobtained. We perform Monte Carlo simulations to compare
performance of proposed estimates and present a discussion based on this
study. Finally, two real data sets are analyzedillustration purposes.

This paper studiesmeasure of symmetry or asymmetry of a
continuous variable undermultiplicative distortion measurement errors
setting.unobservable variable is distorted in a multiplicative fashion by
an observed confounding variable. First, two direct plug-in estimation procedures
are proposed, andempirical likelihood based confidence intervals
are constructed to measuresymmetry or asymmetry ofunobserved
variable. Next, we propose four test statisticstesting whetherunobserved
variable is symmetric or not.asymptotic properties ofproposed
estimators and test statistics are examined. We conduct Monte Carlo
simulation experiments to examineperformance ofproposed estimators
and test statistics. These methods are applied to analyze a real dataset for
an illustration.

Inpaper we propose a new, simple and explicit mechanism
allowing to derive Stein operatorsrandom variables whose characteristic
function satisfies a simple ODE. We applyto study random variables
whichbe represented as linear combinations of (not necessarily independent)
gamma distributed random variables.connection with Malliavin
calculusrandom variables insecond Wiener chaos is detailed. An application
to McKay Type I random variables isoutlined.

Poisson clumping heuristic has lead Aldous to conjecture the
value oforiented first passage percolation onhypercube inlimit
of large dimensions. Aldous’ conjecture has been rigorously confirmed by
Fill and Pemantle (Ann. Appl. Probab. 3 (1993) 593–629) by means of a
variance reduction trick.We present here a streamlined and, we believe, more
natural proof based on ideas emerged instudy of Derrida’s random energy
models.

Given a branching random walk on a set X, we study its extinction
probability vectors q(·,A). Their components areprobability that the
process goes extinct in a fixed A ⊆ X, when starting from a vertex x ∈ X.
set of extinction probability vectors (obtained letting A vary among all
subsets of X) is a subset ofset offixed points ofgenerating function
ofbranching random walk. In particular here we are interested in
cardinality ofset of extinction probability vectors. We prove results
which allow to understand whetherprobability of extinction in a set A is
different fromone of extinction in another set B. In many cases there are
only two possible extinction probability vectors and so far, in more complicated
examples, only a finite number of distinct extinction probability vectors
had been explicitly found. Whether a branching random walk could have an
infinite number of distinct extinction probability vectors was not known. We
apply our results to construct examples of branching random walks with uncountably
many distinct extinction probability vectors.

Recently some nonparametric estimation procedures have been
proposed using kernels and wavelets to estimatecopula function. In this
context, knowing that a copula functionbe expanded in a wavelet basis,
we propose a new nonparametric copula estimation procedure through
waveletsindependent data and times series under an α-mixing condition.
main feature ofestimator is that we make no assumptions ondata
distribution and there is no need to use ARMA–GARCH modelling before
estimatingcopula. Convergence rates forestimator were computed,
showingestimator consistency. Some simulation studies are presented, as
well as analysis of real data sets.

Based on a decomposition of a U-statistic, Nobre, Singer and Silvapulle
(In Beyond Parametrics in Interdisciplinary Research, Festschrift to
P.K. Sen (2008) 197–210 Institute of Mathematical Statistics) proposed a test
forhypothesis thatwithin-treatment variance component in a one-way
random effects model is null, specially useful when very mild assumptions
are imposed onunderlying distributions. We consider a bootstrap version
of that U-test and evaluate its performance via simulation studies in different
scenarios.bootstrap U-test has better statistical properties thanoriginal
test even in small samples. Furthermore, it is easy to implement and has
a low computational cost. We consider two examples with unbalanced small
sample datasets,illustrative purposes.

Inwork, we considerproblem of findingmoments of
a doubly truncated member ofclass of scale mixtures of skew-normal
(TSMSN) distributions. We obtain a general result and then use it to derive
moments incase of doubly truncated versions of skew-normal, skewt,
skew-slash and skew-contaminated normal distributions. Many properties
ofTSMSN family are studied, inference procedures are developed and a
simulation study is performed to assessprocedures. Two applications are
also provided, one of them incontext of censored regression models and
another infield of actuarial sciences.

Birnbaum–Saunders distribution is a flexible and useful
model which has been used in several fields. Inpaper, a new bimodal
version ofdistribution based onalpha-skew-normal distribution is
established. We discuss some of its mathematical and inferential properties.
We consider likelihood-based methods to estimatemodel parameters. We
carry out a Monte Carlo simulation study to evaluateperformance of the
maximum likelihood estimators.illustrative purposes, three real data sets
are analyzed.results indicated thatproposed model outperformed
some existing models inliterature, in special, a recent bimodal extension
ofBirnbaum–Saunders distribution.

This paper considers linear regression models when neither the
response variable norcovariatesbe directly observed, but are measured
with multiplicative distortion measurement errors. To eliminateeffect
caused bydistortion, we propose two calibration procedures:conditional
absolute mean calibration andconditional variance calibration.
Both calibration procedures avoid usingnonzero expectation conditions
imposed onvariables inliterature. Utilizing these calibrated variables,
least squares estimators are obtained, associated with their asymptotic results.
asymptotic normal confidence intervals and empirical likelihood
confidence intervals areproposed. Simulation studies are conducted to
compareproposed calibration procedures and a real example is analyzed
to illustrate our proposed method.

We consider a nonparametric Bayesian approach to estimate the
diffusion coefficient of a stochastic differential equation given discrete time
observations over a fixed time interval. As a prior ondiffusion coefficient,
we employ a histogram-type prior with piecewise constant realisations on
bins forming a partition oftime interval. Specifically, these constants are
realizations of independent inverse Gamma distributed randoma variables.
We justify our approach by derivingrate at whichcorresponding
posterior distribution asymptotically concentrates arounddata-generating
diffusion coefficient.posterior contraction rate turns out to be optimal
for estimation of a Hölder-continuous diffusion coefficient with smoothness
parameter 0 < λ ≤ 1. Our approach is straightforward to implement, as the
posterior distributions turn out to be inverse Gamma again, and leads to good
practical results in a wide range of simulation examples. Finally, we apply
our method on exchange rate data sets.

Inpresent communication,problem of estimating entropy
of a scale mixture of exponential distributions is considered undersquared
error loss. Inadmissibility ofbest affine equivariant estimator(BAEE) is
established by deriving an improved estimator which is not smooth. Using the
integral expression of risk difference (IERD) approach of Kubokawa (The
Annals of Statistics 22 (1994) 290–299), classes of estimators are obtained
which improve uponBAEE.boundary estimator ofclass is the
Brewster and Zidek-type estimator andestimator is smooth. We have
shown thatBrewster and Zidek-type estimator is a generalized Bayes estimator.
As an application of these results, we have obtained improved estimators
forentropy of a multivariate Lomax distribution. Finally, percentage
risk reduction ofimproved estimators forentropy of a multivariate Lomax
distribution is plotted to comparerisk performance ofimproved
estimators.

We study a rumor model from a percolation theory and branching
process point of view. It is defined according tofollowing rules: (1) at time
zero, onlyroot (a fixed vertex oftree) is declared informed, (2) at time
n + 1, an ignorant vertex getsinformation if it is, at a graph distance, at
most Rv of some its ancestral vertex v, previously informed. We present relevant
lower and upper bounds forprobability of that event, according to
distribution ofrandom variables that definesradius of influence
of each individual. We work with (homogeneous and non-homogeneous)
Galton–Watson branching trees and spherically symmetric trees which includes
homogeneous and k-periodic trees. Wepresent boundsthe
expected size ofconnected component insubcritical casehomogeneous
trees and homogeneous Galton–Watson branching trees.

This paper deals with branching processes in varying environment
with selection, whereoffspring distribution depends ongeneration and
every particle has a random fitness whichonly increase along genealogical
lineages (descendants with small fitness do not survive). We view the
branching process in varying environment (BPVE) as a particular example
of branching random walk. We obtain conditions forsurvival or extinction
of a BPVE (with or without selection), using fixed point techniques for
branching random walks. These conditions rely only onfirst and second
moments ofoffspring distributions. Our resultsbe interpreted in terms
of accessibility percolation on Galton-Watson trees. In particular, we obtain
that there is no accessibility percolation on almost every Galton-Watson tree
whereexpected number of offspring grows sublinearly in time, while superlinear
growths allows percolation.result is in agreement with what
was founddeterministic trees in Nowak and Krug (Europhysics Letters
101 (2013) 66004).

Inarticle, we investigate consistency and asymptotic normality
ofmaximum likelihood andposterior distribution ofparameters
incontext of state space stochastic differential equations (SDEs).We then
extend our asymptotic theory to random effects models based on systems of
state space SDEs, covering both independent and identical and independent
but non-identical collections of state space SDEs.Weaddress asymptotic
inference incase of multidimensional linear random effects, and in situations
wheredata are available in discretized forms. It is important to note
that asymptotic inference, either inclassical or inBayesian paradigm,
has not been hitherto investigated in state space SDEs.

existence of an invariant probability measure is provena
class of solutions of stochastic differential equations with finite delay. This
is done, innon-Markovian setting, usingcluster expansion method,
from Gibbs field theory. It holdssmall perturbations of ergodic diffusions.

This paper introduces two classes of binomial integer-valued
ARCH models with dynamic survival probabilities, each of which is controlled
by a stochastic recurrence equation. Stationarity and ergodicity of the
process are established, and stochastic properties are given. Conditional least
squares and conditional maximum likelihood estimators forparameters
of interest are considered, and their large-sample properties are established.
performances of these estimators are compared via simulation studies.
Finally, we demonstrateusefulness ofproposed models by analyzing
real datasets.

Conducting sequencing experiments with good statistical properties
and low cost is a crucial challengeboth researchers and practitioners.
main reason forchallenge iscombinatorial nature of
problem andpossible conflicts among objectives.problem was
addressed by proposing a mathematical programming formulation aimed at
generating minimum-cost run orders withbest statistical properties2k
full-factorial and fractional-factorial designs.approach performance is
evaluated using designs of up to 64 experiments with different levels of resolution.
results indicate thatapproachyield optimal or sub-optimal
solutions, depending onobjectives establisheda given design matrix.

Inpaper, we considerproblem of simultaneously estimating
parameters of independent Poisson distributions inpresence of possibly
unbalanced sample sizes under weighted standardized squared error loss.
A class of heterogeneous Bayesian shrinkage estimators that utilizeunbalanced
nature of sample sizes is proposed. To provide a theoretical justification,
we first derive a necessary and sufficient conditionan estimator
inclass to be proper Bayes and hence admissible and then obtain sufficient
conditionsminimaxity that are compatible withadmissibility
condition. Heterogeneous and homogeneous shrinkage estimators are compared
by simulation. Several estimation methods are applied to data relating
tostandardized mortality ratio.

We consider multiple imputation (MI)unbalanced ranked set
samples (URSS) by considering them as data sets with missing values. We
replace each missing value with a set of plausible values drawn from a predictive
distribution that representsuncertainty aboutappropriate value
to impute. Usingstructure ofMI dataset, we develop algorithms that
imitatestructure of URSS to carry outdesired statistical inference.We
provide results forconvergence ofempirical distribution functions of
imputed samples topopulation distribution function, under both URSS
and simple random sampling (SRS). We obtainvariances ofimputed
URSS, andexpected values ofvariance estimators. Westudy the
problem of quantile estimation using an imputed URSS and propose a hybrid
method based onbootstrap and imputation of URSS data. We apply our
results to estimatemean and quantiles ofmercury in contaminated fish
under perfect and imperfect URSS.

Partially linear generalized single index models are widely used
and have attracted much attention inliterature. However, whencovariates
are subject to measurement error,problem is much less studied.
Onother hand, instrumental variables are important elements in studying
many errors-in-variables problems. We userelation betweenunobservable
variables andinstruments to devise consistent estimatorspartially
linear generalized single index models with binary response. We establish
consistency, asymptotic normality ofestimator and illustrate the
numerical performance ofmethod through simulation studies and a data
example. Despiteconnection to (Scand. J. Statist. 42 (2015) 104–117) in
its general layout,mathematical derivations are much more challenging
incontext studied here.

We provide a general resultfinding Stein operatorsthe
product of two independent random variables whose Stein operators satisfy
a certain assumption, extending a recent result of (Journal of Mathematical
Analysis and Applications 469 (2019) 260–279).framework applies to
non-centered normal and non-centered gamma random variables, as well as
a general sub-family ofvariance-gamma distributions. Curiously, there is
an increase in complexity inStein operatorsproducts of independent
normals as one moves,example, from centered to non-centered normals.
As applications, we give a simple derivation ofcharacteristic function of
product of independent normals, and provide insight into whyprobability
density function ofdistribution is much more complicated in the
non-centered case thancentered case.

A test of fit forexponential distribution is presented, which
is based on transformed observations and a new estimator forscale parameter.
asymptotic null distribution oftest statistic is obtained and
consistency oftest is discussed. Monte Carlo simulation results on a
power comparison studythatproposed test is competitive under the
considered families of alternatives and sample sizes.

Followingmethodology developed by (Comput. Math. Appl.
33 (1997) 81–104), we define a discrete version of gradient vector and associated
line integral along arbitrary path connecting two nodes of uniform
grid. An exponential representation of joint survival function of bivariate discrete
non-negative integer-valued random variables in terms of discrete line
integral is established. We apply it to generate a discrete analogue of the
Sibuya-type aging property, incorporating many classical and new bivariate
discrete models. Several characterizations and closure properties ofclass
of bivariate discrete distributions are presented.

market weight of a stock is its capitalization (cap) divided
bytotal market cap. Rank these weights from top to bottom.capital
distribution curve is a plot of weights versus ranks. ForUS stock market,
it is linear on a double logarithmic scale, and stable with respect to time
(Stochastic Portfolio Theory (2002) Springer).property has been captured
by models with rank-dependent dynamics: Each stock’s cap logarithm
is a Brownian motion with drift and diffusion coefficients depending on its
current rank (Probability Theory and Related Fields 147 (2010) 123–159).
However, short-term stock movements have heavy tails. Oneadd jumps
to Brownian motions to capture this. Observed time stability follows from a
long-term stability result, stated and proved here. Via simulations, we find
which properties of continuous models are preserved after adding jumps.

Consider a renewal–reward process SN(t)
= N(t)
k=1 Xk and let
{τn} beinterarrival times. It is well known that, under regularity conditions,
SN(t) is asymptotically Gaussian provided Xn and τn have finite second
moment. However, in modelling risk processes or heavy traffic networks,
assumption offiniteness ofsecond moment may not be compatible.
Also,independency ofprocesses {Sn} and {N(t)} might be not
realistic. Insituation, heavy-tailed distributions arise as a proper alternative
and dependency between τn andreward Xn should be allowed. By
making use ofMallows–Wasserstein distance we derive CLT type results
for heavy-tailed renewal–reward dependent processes. Applications to risk
processes and heavy traffic networks are exhibited.

Inwork, we investigateasymptotic behavior ofextremes
of a multivariate data by usingReduced Ordering Principle (Rordering).
When,sup-norm is used, we revealinterrelation between
R-ordering principle and Marginal Ordering Principle (M-ordering). The
asymptotic behavior ofmaximum sup-norms corresponding tobivariate
data is completely determined. Finally, an application to real data illustrates
and corroboratestheoretical results.

We provereduction principleasymptotics of functionals
of vector random fields with weakly and strongly dependent components.
These functionalsbe used to construct new classes of random fields with
skewed and heavy-tailed distributions. Contrary tocase of scalar longrange
dependent random fields, it is shown thatasymptotic behaviour of
such functionals is not necessarily determined byterms at their Hermite
rank.results are illustrated by an application tofirst Minkowski functional
ofStudent random fields. Some simulation studies based on the
theoretical findings arepresented.

We briefly expose some key aspects oftheory and use of dispersion
models,which Bent Jørgensen played a crucial role as a driving
force and an inspiration source. Starting withgeneral notion of dispersion
models, built using minimalistic mathematical assumptions, we specialize in
two classes of families of distributions with different statistical flavors: exponential
dispersion and proper dispersion models.construction of dispersion
models involvessolution of integral equations that are, in general,
untractable. These difficulties disappear when more mathematical structure
is assumed: it reduces tocalculation of a moment generating function or
of a Riemann–Stieltjes integral forexponential dispersion andproper
dispersion models, respectively. A new techniqueconstructing dispersion
models based on characteristic functions is introduced turningintegral
equations above into a tractable convolution equation and yielding examples
of dispersion models that are neither proper dispersion nor exponential dispersion
models. A corollary is thatcardinality of regular and non-regular
dispersion models are both large.
Some selected applications are discussed including exponential families
non-linear models (for which generalized linear models are particular cases)
and several modelsclustered and dependent data based on a latent Lévy
process.

Accurately quantifying extreme rainfall is important fordesign
of hydraulic structures,flood mapping and zoning anddisaster
management. In order to produce maps of estimates of 25-year rainfall return
levels in Brazil, we selected 893 shorter and 104 longer rainfall time series
fromAgência Nacional de Águas (ANA), and appliedframework of
extreme value theory. Care was needed to reduceimpact of poor data.
Estimates ofshape parameter ofextreme-value model fitted to rainfall
data are typically biased, so we discuss an empirical correction that takes
into account not onlysample-size bias, buta so-called penultimate
approximation that we use to inform a Bayesian spatial latent variable model
forannual rainfall maxima.model accountssubtle patterns of
spatial variation indata and provides plausible return level estimates.

We present a general expression that allowscalculation of both
n
−2 asymptotic covariance matrices ofmaximum likelihood estimator
(MLE) andfirst-order bias corrected MLE, where n issample size.
formula is presented in a matrix notation which has numerical advantages
since it requires only simple operations on matrices and vectors. The
usefulness offormula is to construct better Wald statistics. We apply our
findings to dispersion models and develop simulation studies whichthat
modification inWald statistic effectively removes size distortions of the
type I error probability with no power loss.illustrative purposes, a real
data application is considered to support our theoretical results.

Ecologists have had an ongoing interest in a variance to mean
power law that governsclustering of individuals of animal and plant
species.same power law has been reported from disparate biological,
physical and mathematical systems, andcharacterizes a family of statistical
distributions known asTweedie exponential dispersion models.
Its widespread appearancebe explained by fundamental statistical convergence
effects on random data that cause this, and related, power laws to
emerge and provide mechanistic insight into its origin, as well asorigin of
1/f noise, multifractality and other phenomena attributable to self-organized
criticality. A meta-analysis of ecological field data was conducted here to examine
how such statistical convergence might affectpower law. These
findings provided conjectural insight into a form of self-organized criticality,
driven and modulated bystatistical convergence of random data, which
could underliepower law’s emergence.

A common assumption instandard tobit model isnormality
forerror distribution. However, asymmetry and bimodality may be
present and alternative tobit models must be used in such cases. Inpaper,
we propose a tobit model based onclass of log-symmetric distributions,
which includes as special cases heavy/light tailed distributions and
bimodal distributions.We implement a likelihood-based approachparameter
estimation and consider a type of residual. We then discussproblem
of performing hypothesis tests withinproposed class by usinglikelihood
ratio and gradient statistics, which are particularly convenienttobit
models, as they do not requireinformation matrix. An elaborate Monte
Carlo study is carried outevaluatingperformance ofmaximum
likelihood estimates,likelihood ratio and gradient tests andempirical
distribution ofresiduals. Finally, we illustrateproposed methodology
withuse of a real data set.

Many extreme events are characterized by being always susceptible
to outside influences that will modify their behavior at some point in time.
change point tool has been used in statistical models to detect when these
changes occur.paper presents a model based on a Bayesian approach
that describesbehavior of extreme data regarding river quota, which may
present more than one change point. In each one ofregimes,GEV
distribution is adjusted and each GEV parameter of each regime is written in
function of presence of covariates. Inapplications proposed here,results
showed thatmodel was able to accurately estimateactual amount
of change points inseries, andshowed that it was extremely important
to consider them inanalysis, since it was verified that afterchange
of regime,levels of return have changed considerably.results were
also able towhich monthsoccurrence of an extreme event is greater.

irregular fluctuations of solar flare emissions, as determined
from terrestrial neutron monitors, remains poorly understood. These records
empirically revealed a temporally-related variance to mean power law, 1/f
noise and a non-Gaussian distribution, all features indicative of self-organized
criticality, a theory of how derministic dynamical systemsspontaneously
evolve to unstable states that express erratic changes.non-Gaussian distribution
found here approximated a Tweedie compound Poisson exponential
dispersion model, a statistical distribution characterized by a variance tomean
power law that itselfimply 1/f noise. Tweedie exponential dispersion
models serve a primary role in statistical theory as fociweak convergence
for a wide range of random distributions, a role which supports an alternative
conjecture to explainsolar flare fluctuations as being based on random
processes rather than a deterministic system.

Birnbaum–Saunders distribution has been widely used to
model reliability and fatigue data. Inpaper, we propose a regression of
generalized linear models type based on a new bivariate Birnbaum–Saunders
distribution.is parameterized in terms of its means and allows data to be
described in their original scale. We estimatemodel parameters and carry
out inference withmaximum likelihood method. A case study with realworld
reliability data is conductedmotivating our investigation, illustrating
potential applications ofproposed results. We obtain a predictive
model whichbe a useful addition totool-kit of diverse practitioners,
reliability engineers, applied statisticians, and data scientists.

One ofindicatorsevaluatingcapability of a process potential
and performance in an effective way isprocess capability index
(PCI). It is of great significance to quality control engineers as it quantifies
relation betweenactual performance ofprocess andpre-set
specifications ofproduct. Most oftraditional PCIs performed well
when process followsnormal behaviour. Inarticle, we consider a
process capability index, Cpk, suggested by Kane (Journal of Quality Technology
18 (1986) 41–52) whichbe usednormal random variables.
objective ofarticle is three fold: First, we address different methods
of estimation ofprocess capability index Cpk from frequentist approaches
fornormal distribution. We briefly describe different frequentist
approaches, namely, maximum likelihood estimators, least squares and
weighted least squares estimators, maximum product of spacings estimators,
Cramèr–von-Mises estimators, Anderson–Darling estimators and Right-
Tail Anderson–Darling estimators and compare them in terms of their mean
squared errors using extensive numerical simulations. Second, we compare
three parametric bootstrap confidence intervals (BCIs) namely, standard bootstrap,
percentile bootstrap and bias-corrected percentile bootstrap. Third, we
consider Bayesian estimation under squared error loss function using normal
priorlocation parameter and inverse gammascale parameter for
considered model. Monte Carlo simulation study has been carried out to
compareperformances ofclassical BCIs and highest posterior density
(HPD) credible intervals of Cpk in terms of average widths and coverage
probabilities. Finally, two real data sets have been analyzedillustrative
purposes.

Estimation of microorganism concentration in ballast water tanks
is important to evaluate and possibly to preventintroduction of invasive
species in stable ecosystems.such purpose,number of organisms in
ballast water aliquots must be counted and used to estimate their concentration
with some precision requirement. Poisson and negative binomial models
have been employed to describeorganism distribution intank, but determination
of sample sizes required to generate estimates with pre-specified
precision is still not well established. A Bayesian approach is a flexible alternative
to accommodate adequate models that account forheterogeneous
distribution oforganisms and may provide a sequential way of enhancing
estimation procedure by updatingprior distribution alongballast
water discharging process. We adopt such an approach to compute sample
sizes required to construct credible intervals obtained via two optimality criteria
that have not been employed incontext. Such intervals may be used
indecision with respect to compliance withD-2 standard ofBallast
Water Management Convention. Weconduct a simulation study to
verify whethercredible intervals obtained withproposed sample sizes
satisfyprecision criteria.

Proportional hazards (PH), proportional odds (PO) and accelerated
failure time (AFT) models have been widely used to deal with survival
data in different fields of knowledge. Despite their popularity, such models
are not suitable to handle survival data with crossing survival curves. Yang
and Prentice (2005) proposed a semiparametric two-sample approach, denoted
here asYP model, allowinganalysis of crossing survival curves
and includingPH and PO configurations as particular cases. In a general
regression setting,present work proposes a fully likelihood-based
approach to fitYP model.main idea is to modelbaseline hazard
viapiecewise exponential (PE) distribution.approach shares the
flexibility ofsemiparametric models andtractability ofparametric
representations. An extensive simulation study is developed to evaluate the
performance ofproposed model. We demonstrate how useful isnew
method throughanalysis of survival times related to patients enrolled in a
cancer clinical trial. Finally, an R package called YPPE was developed to fit
proposed model.simulation results indicate that our model performs
wellmoderate sample sizes ingeneral regression setting. A superior
performance isobserved with respect tooriginal YP model designed
fortwo-sample scenario.

Inpaper, we introducerandom deterioration rate model
with measurement error in order to incorporatevariability among different
components.motivation behindrandom variable model is to
capturerandomness inindividual differences acrosspopulation.
This model incorporates only sample uncertainty ofdegradation, and no
temporal variability is included.measurement error models appear to
overcomeproblem.random rate analysis is based on repeated measurements
of failure sizes generated by a degradation process over time in a
components population. Some characteristics ofrandom deterioration rate
model based oninverse Gaussian distribution and subject to measurement
error, are examined. We carry out simulation studies to (i) assessperformance
ofmaximum likelihood estimates obtained throughGaussian
quadrature along with Quasi-Newton optimization method; and (ii) examine
effects of model misspecification onmodel selection criteria’s performance,
as well as onlifetime prediction’s accuracy and precision. The
potentiality ofproposed model is illustrated through two real data sets.

continuous time Markov process considered inpaper belongs
to a class of population models with linear growth and catastrophes.
There,catastrophes happen atarrival times of a Poisson process, and
at each catastrophe time, a randomly selected portion ofpopulation is
eliminated. Forpopulation process, we derive an asymptotic upper bound
formaximum value and provelocal large deviation principle.

Inarticle we study a small random perturbation of a linear recurrence
equation. If allroots of its corresponding characteristic equation
have modulus strictly less than one,random linear recurrence goes exponentially
fast to its limiting distribution intotal variation distance as time
increases. By assuming that allroots of its corresponding characteristic
equation have modulus strictly less than one and rather mild conditions, we
prove thatconvergence happens as a switch-type, i.e., there is a sharp
transition inconvergence to its limiting distribution.fact is known as
a cut-off phenomenon incontext of stochastic processes.

Central Limit Theorem (CLT)additive functionals of
Markov chains is a well-known result with a long history. Inpaper, we
present applications to two finite-memory versions ofElephant Random
Walk, solving a problem from Gut and Stadtmüeller (2018). Wepresent
a derivation ofCLTadditive functionals of finite state Markov chains,
which is based on positive recurrence,CLTIID sequences and some
elementary linear algebra, and which focuses on characterization ofvariance.

Motivated by models in engineering andbiology we determine
in closed formprobability density function ofangle shaped by
two random chords in a fixed disc. Our main result focus onevent in
whichintersection locates insidefixed disc and establishes a sine law.

For -n-coalescents with mutation, we analyseminimal observable
clade size On.a given individual i ∈ {1, . . . , n},minimal observable
clade size On(i) is 1 plusnumber of other individuals j such that
every non-singleton mutation inherited by i isinherited by j .We provide
asymptotics of On as n→∞and a recursionall moments of Onfinite
n.statistic gives an upper bound forminimal clade size (Advances
in Applied Probability 37 (2005) 647–662) which is not observable in real
data (in contrast to On). In applications to genetics, it has been shown to be
useful to lower classification errors in genealogical model selection (Freund
and Siri-Jégousse, 2020).

Recently, there has been a growing interest in integer-valued time
series models, including integer-valued autoregressive (INAR) models and
integer-valued generalized autoregressive conditional heteroscedastic (INGARCH)
models, but only a few of themdeal with data onfull set of
integers, that is, Z = {. . . ,−2,−1, 0, 1, 2, . . .}. Although some attempts have
been made to deal with Z-valued time series, these models do not provide
enough flexibility in modeling some specific integers (e.g., 0, ±1). A symmetric
Skellam INGARCH(1, 1) model was proposed inliterature, but it
only considered zero-mean processes, which limits its application. We first
extendsymmetric Skellam INGARCH model to an asymmetric version,
whichdeal with non-zero-mean processes. Then we propose a modified
Skellam model which adopts a careful treatment on integers 0 and ±1 to satisfy
a special feature ofdata. Our models are easy-to-use and flexible.
maximum likelihood method is used to estimate unknown parameters
andlog-likelihood ratio test statistic is providedtestingasymmetric
model againstmodified one. Simulation studies are given to evaluate
performances ofparametric estimation and log-likelihood ratio test. A real
data example ispresented to demonstrate good performances of newly
proposed models.

Inpaper, we consider a semiparametric regression model
whereerror follows a scale mixture of Gaussian distributions.purpose
is to estimatetarget function which is assumed to belong to some
class of functions usingEM algorithm and approximations via P-splines
and B-splines. We illustrateproposed methodology through several simulation
studies. Other forms of function approximation arestudied,
namely Fourier and wavelet expansions.

In many applications, data exhibit skewness and inpaper we
present a new family of density functions modeling skewness based on a
transformation, analogous to those of location and scale. Here we note that
location will always refer to mode. Hence, in order to model data to include
shape, we need only to find a family of densities exhibiting a variety of
shapes, since weobtainother three properties viatransformations.
chosen class of densities withvariety of shape is, we argue, the
simplest available. Illustrations including regression and time series models
are given.

Inpaper, we establish some resultsmultivariate selection
scale-mixtures of normal distributions with arbitrary mixing variable. First,
we discuss their stochastic representation in terms of multivariate selection
normal distributions. Next,conditional distributions as well asfirst
two moments of multivariate selection scale-mixtures of normal distributions
are obtained whenselection set is an arbitrary rectangle inqdimensional
Euclidean space of Rq .unified skew-scale mixture of normal
(SUSMN) distributions are subsequently discussed as a special case. As
a subclass of SUSMN distributions,class of unified skew-symmetric generalized
hyperbolic (SUSGH) distributions are studied in detail. Finally, we
show that our resultsbe used to obtain moments of L-statistics and of
multivariate concomitants from multivariate scale-mixtures of normal distributions.

This work deals with problem of estimatingodds using judgment
post stratification (JPS) sampling design. Several estimators ofodds
are described andasymptotic normality of each of them is established.
Monte Carlo simulation study is then used to compare different estimators of
odds inJPS withstandard estimator in simple random sampling
(SRS) with replacementboth perfect/imperfect ranking andboth JPS
data with/without empty strata.comparison results indicate thatestimators
developed herebe highly more efficient than their SRS counterpart
in some certain circumstances. Finally, a real dataset fromthird National
Health and Nutrition Examination Survey (NHANES III) is employed
for illustration purposes.

Suppose X1, . . . , Xn is a random sample from a bounded and decreasing
density f0 on [0,∞). We are interested in estimating such f0, with
special interest in f0(0).problem is encountered in various statistical
applications and has gained quite some attention instatistical literature. It
is well known thatmaximum likelihood estimator is inconsistent at zero.
This has led several authors to propose alternative estimators which are consistent.
As any decreasing densitybe represented as a scale mixture of
uniform densities, a Bayesian estimator is obtained by endowingmixture
distribution withDirichlet process prior. Assumingprior, we derive
contraction rates ofposterior density at zero by carefully revising arguments
presented in Salomond (Electronic Journal of Statistics 8 (2014) 1380–
1404). Several choices of base measure are numerically evaluated and compared.
In a simulation various frequentist methods and a Bayesian estimator
are compared. Finally,Bayesian procedure is applied to current durations
data described in Slama et al. (Human Reproduction 27 (2012) 1489–1498).

A Bayesian nonparametric estimator to entropy is proposed. The
derivation ofnew estimator relies on usingDirichlet process and
adaptingwell-known frequentist estimators of Vasicek (Journal of Royal
Statistical Society B 38 (1976) 54–59) and Ebrahimi, Pflughoeft and Soofi
(Statistics & Probability Letters 20 (1994) 225–234). Several theoretical
properties, such as consistency, ofproposed estimator are obtained. The
quality ofproposed estimator has been investigated through several examples,
in which it exhibits excellent performance.

aim ofnote is to give an elegant proof of a result due to
E. G. Olds which concernsdensity distribution ofsum of independent
uniform random variables non-identically distributed.proof uses
both analytical and combinatorial properties of Dirac distributions and their
convolutions.method is new andapply to other situations.

present article represents a review ofgeometric generated
family of distributions. Based onfamily of distribution, several distributions
are proposed.familybe proposed by usingcompounding
concept of zero truncated geometric distribution with any other model or
family of distributions. Here, we provide a complete survey onfamily of
distributions andlistedcontributory related research work, their submodels,
hazard rates, and utilized real datasets. Weaddress 10 power
series distributions, 60 distributions based ongeometric family of distribution.
These numbers showimportance ofgeometric family of
distribution.

Bayesian statistical methodology has become highly popular in a
myriad of applications overpast several decades. In Bayesian statistics, it
is often required to draw samples from intractable probability distributions.
Markov chain Monte Carlo (MCMC) algorithms are common methods of obtaining
samples from these distributions. When an MCMC algorithm is used,
it is important to be able to obtain an answer toquestion of how many iterations
chain must run before it is “close enough” to its target distribution
to allow approximate sampling fromdistribution. Several methods of approaching
this question exist inliterature. Some rely onoutput of the
chain, and some are based on Markov chain theory. These techniques suffer
from major practical limitations.work provides a computational method
of boundingmixing time of a Metropolis–Hastings algorithm.approach
extendswork of Spade (Statistics and Computing 26 (2016) 761–
781) and Spade (Markov Processes and Related Fields 26 (2020) 487–516)
to general versions ofMetropolis–Hastings algorithm, while examining
convergence behavior of such samplers under symmetric and asymmetric
proposal densities.

Nair, Sankaran and John (Metron 76 (2018) 133–153) have defined
and studiedproperties of reliability functions in terms of copulas. In
present paper, we investigateutility of such functions in inferring the
time-dependent association of bivariate distributions. We considerClayton
measure of association forstudy. A general expression formeasure
in terms ofgenerator of Archimedean copulas is given, and a method
of finding nature of association usinggenerators is provided. We derive
relationship ofassociation measure withageing property of the
distribution, associated withgenerator.We analyze howhazard rate of
survival copulasbe utilized in studyingassociation between two random
variables. Applications ofresults in real life situations are discussed.

We analyzefluctuations of incomplete U-statistics over a triangular
array of independent random variables. We give criteriaa Central
Limit Theorem (CLT,short) to hold insense that we prove that an
appropriately scaled and centered version ofU-statistic converges to a
normal random variable. Our method of proof relies on a martingale CLT. An
application, a CLT forhitting timerandom walks on random graphs,
will be presented in Löwe and Terveer (2020).

For modelling unbounded count data, Poisson distribution is a natural
choice. However, count data arising in various fields of scientific research
are often under-reported. In such situations, inference carried out onbasis
of Poisson model will result in biased parameter estimates and suboptimal
tests. A modified Poisson model is developed to accommodatepossible
undercount.model-identifiability a double sampling scheme of data collection
has been adopted.focus ofpaper is to develop asymptotically
optimal tests forPoisson mean in presence of undercount. Simulation
study is conducted to compareperformance oftests with respect to
level and power andto investigateimpact of ignoring undercount on
each oftests.findings are validated using real life data.

Assume we observe a finite number of inspection times together
with information on whether a specific event has occurred before each of
these times. Suppose replicated measurements are available on multiple event
times.set of inspection times, includingnumber of inspections, may
be differenteach event.is known as mixed case interval censored
data. We consider Bayesian estimation ofdistribution function of the
event time while assuming it is concave. We provide sufficient conditions
onprior such thatresulting procedure is consistent fromBayesian
point of view. Weprovide computational methodsdrawing from the
posterior and illustrateperformance ofBayesian method in both a
simulation study and two real datasets.

Inpaper, we consider a modeldata analysis with measurement
errors.main objective ofwork is to develop statistical inference
tools, such as parameter estimation and hypothesis tests in a linear
functional relationship with replicated observations. Forpurpose, we use
maximum likelihood method inpresence of incidental parameters, and
unbiased estimating equations approach. Both approaches lead to explicit
expressions forasymptotic covariance matrices ofestimators of the
model parameters. A simulation study is performed to assessempirical
behavior of estimators and of a Wald statistic.methodology is illustrated
with a real data set.

We compute higher derivatives ofFréchet function on spheres
with an absolutely continuous and rotationally symmetric probability distribution.
Consequences include (i) a practical condition to test ifmode of
symmetric distribution is a local Fréchet mean; (ii) a central limit theorem
on spheres with practical assumptions and an explicit limiting distribution;
and (iii) an answer toquestion of whethersmeary effectoccur on
spheres with absolutely continuous and rotationally symmetric distributions:
withmethod presented here, itin dimension at least 4.

We consider a version of continuum long-range percolation on
finite boxes of Rd in whichvertex set is given bypoints of a Poisson
point process and each pair of two vertices at distance r is connected with
probability proportional to r
−sa certain constant s.We exploregraphtheoretical
distance inmodel.aim ofpaper is tothat this
random graph model undergoes phase transitions at values s = d and s = 2d
in analogy to classical long-range percolation on Zd , by using techniques
which are based on an analysis ofunderlying Poisson point process.

This is a study ofbehavior under partition ofsample space
of three multivariate distributions: multinomial, multinomial-Dirichlet, and
Dirichlet. A general theorem is given, of which all three are special cases.

class of generalized hyperbolic (GH) distributions is generated
by a mean-variance mixture of a multivariate Gaussian with a generalized
inverse Gaussian (GIG) distribution.rich family of GH distributions
includes some well-known heavy-tailed and symmetric multivariate distributions,
includingNormal Inverse Gaussian and some members of the
family of scale-mixture of skew-normal distributions.class of GH distributions
has received considerable attention in finance and signal processing
applications. Inpaper, we proposelikelihood ratio (LR) test to test
hypotheses aboutskewness parameter of a GH distribution. Due to the
complexity oflikelihood function,EM algorithm is used to find the
maximum likelihood estimates both incomplete model andreduced
model.comparative purposes and due to its simplicity, weconsider
Gradient (G) test. A simulation study shows thatLR and G tests are
usually able to achievedesired significance levels andtesting power
increases asasymmetry increases.methodology developed inpaper
is applied to two real datasets.
